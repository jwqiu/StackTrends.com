import sys, os
import pandas as pd
from sklearn.model_selection import train_test_split

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from python_scraper.connect import get_conn
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
from sklearn.utils.class_weight import compute_class_weight
import numpy as np
import random

# å›ºå®šéšæœºç§å­ï¼Œä¿è¯ç»“æœå¯å¤ç°
SEED = 42
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
np.random.seed(SEED)
random.seed(SEED)

# ç¡®ä¿æ¯æ¬¡è®¡ç®—ä¸€è‡´ï¼ˆä½†ç•¥å¾®å½±å“æ€§èƒ½ï¼‰
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# ---------------------------
# 1ï¸âƒ£ è¯»å–æ•°æ®åº“æ•°æ®
# ---------------------------
# conn = get_conn()
# verify_cur = conn.cursor()
# verify_cur.execute("""
#     SELECT job_id, job_title, job_des, job_level
#     FROM jobs_filtered
#     WHERE job_level IS NOT NULL;
# """)
# rows = verify_cur.fetchall()
# # å®‰å…¨æ£€æŸ¥
# if verify_cur.description is None:
#     raise RuntimeError("âš ï¸ SQL æ‰§è¡Œå¤±è´¥æˆ–æ²¡æœ‰è¿”å›ç»“æœï¼Œè¯·æ£€æŸ¥å­—æ®µåæ˜¯å¦æ­£ç¡®ã€‚")
# colnames = [desc[0] for desc in verify_cur.description]
# verify_cur.close()
# conn.close()

# df = pd.DataFrame(rows, columns=colnames)
# print(f"æ€»æ ·æœ¬æ•°: {len(df)}")
# print(df['job_level'].value_counts())

# ---------------------------
# 2ï¸âƒ£ æŒ‰ç±»åˆ«åˆ†å±‚åˆ’åˆ†æ•°æ®é›†
# ---------------------------
# å…ˆåˆ’åˆ† train (0.6) vs temp (0.4)
# train_df, temp_df = train_test_split(
#     df,
#     test_size=0.4,
#     stratify=df['job_level'],
#     random_state=42
# )

# # å†åˆ’åˆ† temp -> val/test å„ 0.2
# val_df, test_df = train_test_split(
#     temp_df,
#     test_size=0.5,
#     stratify=temp_df['job_level'],
#     random_state=42
# )

# ---------------------------
# 3ï¸âƒ£ æ£€æŸ¥æ¯ä¸ªå­é›†çš„åˆ†å¸ƒ
# ---------------------------
# def show_distribution(label, data):
#     print(f"\nğŸ“Š {label} é›†ç±»åˆ«åˆ†å¸ƒ:")
#     print(data['job_level'].value_counts())

# show_distribution("Train", train_df)
# show_distribution("Validation", val_df)
# show_distribution("Test", test_df)

# ---------------------------
# 4ï¸âƒ£ å¯é€‰ï¼šä¿å­˜æˆ–ä¼ é€’å˜é‡
# ---------------------------
# train_df, val_df, test_df å¯ç›´æ¥ç”¨äºæ¨¡å‹è®­ç»ƒ
# ä¾‹å¦‚ï¼š
# train_df.to_csv("train_data.csv", index=False)
# val_df.to_csv("val_data.csv", index=False)
# test_df.to_csv("test_data.csv", index=False)


# ---------------------------
# 1ï¸âƒ£ æ•°æ®å‡†å¤‡
# ---------------------------
# å‡è®¾ä½ å·²æœ‰ train_df, val_df, test_df
# æ¯ä¸ªæœ‰åˆ—: ['job_title', 'job_des', 'job_level']
# å¦‚æœæ–‡æœ¬æ˜¯ job_title + job_des æ‹¼ä¸€èµ·æ›´å¥½
# def combine_text(df):
#     return (df['job_title'].fillna('') + ' ' + df['job_des'].fillna('')).tolist()

# train_texts = combine_text(train_df)
# val_texts = combine_text(val_df)
# test_texts = combine_text(test_df)

data = torch.load("model_pipeline/embeddings/embeddings.pt")

# ---------------------------
# 2ï¸âƒ£ æ–‡æœ¬å‘é‡åŒ– (Embedding)
# ---------------------------

# æ ‡ç­¾ç¼–ç 
le = LabelEncoder()
train_labels = torch.tensor(le.fit_transform(data["train_labels"]), dtype=torch.long)
val_labels   = torch.tensor(le.transform(data["val_labels"]), dtype=torch.long)
test_labels  = torch.tensor(le.transform(data["test_labels"]), dtype=torch.long)

train_emb = data["train_emb"]
val_emb   = data["val_emb"]
test_emb  = data["test_emb"]

# ---------------------------
# 3ï¸âƒ£ æ„å»º DataLoader
# ---------------------------
batch_size = 32
train_loader = DataLoader(TensorDataset(train_emb, train_labels), batch_size=batch_size, shuffle=True)
val_loader = DataLoader(TensorDataset(val_emb, val_labels), batch_size=batch_size)
test_loader = DataLoader(TensorDataset(test_emb, test_labels), batch_size=batch_size)

# ---------------------------
# 4ï¸âƒ£ å®šä¹‰ç®€å• MLP æ¨¡å‹
# ---------------------------
class MLPClassifier(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_classes):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        return self.net(x)

input_dim = train_emb.shape[1]
model = MLPClassifier(input_dim=input_dim, hidden_dim=128, num_classes=len(le.classes_))
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# ---------------------------
# 5ï¸âƒ£ è®­ç»ƒ
# ---------------------------
# criterion = nn.CrossEntropyLoss()

# è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æƒé‡
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(train_labels.numpy()),
    y=train_labels.numpy()
)
class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)

# ä½¿ç”¨åŠ æƒæŸå¤±å‡½æ•°
criterion = nn.CrossEntropyLoss(weight=class_weights)

optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
epochs = 50            # æœ€å¤§è®­ç»ƒè½®æ•°
patience = 10           # è¿ç»­å¤šå°‘è½®éªŒè¯é›†ä¸æå‡å°±åœæ­¢
best_val_loss = float('inf')
patience_counter = 0

for epoch in range(epochs):
    # ====== è®­ç»ƒé˜¶æ®µ ======
    model.train()
    total_loss = 0
    for X_batch, y_batch in train_loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        optimizer.zero_grad()
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    avg_train_loss = total_loss / len(train_loader)

    # ====== éªŒè¯é˜¶æ®µ ======
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for X_val, y_val in val_loader:
            X_val, y_val = X_val.to(device), y_val.to(device)
            outputs = model(X_val)
            loss = criterion(outputs, y_val)
            val_loss += loss.item()

    avg_val_loss = val_loss / len(val_loader)

    print(f"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}")

    # ====== æ—©åœé€»è¾‘ ======
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        patience_counter = 0
        save_dir = "model_pipeline"
        save_path = os.path.join(save_dir, "best_model.pt")
        torch.save(model.state_dict(), save_path)  # ä¿å­˜å½“å‰æœ€ä¼˜æ¨¡å‹
    else:
        patience_counter += 1
        print(f"âš ï¸ No improvement for {patience_counter} epoch(s)")
        if patience_counter >= patience:
            print("â¹ï¸ Early stopping triggered!")
            break
# ---------------------------
# 6ï¸âƒ£ è¯„ä¼°
# ---------------------------
def evaluate(model, loader, y_true):
    model.eval()
    preds = []
    with torch.no_grad():
        for X_batch, _ in loader:
            X_batch = X_batch.to(device)
            outputs = model(X_batch)
            preds.extend(outputs.argmax(dim=1).cpu().numpy())
    return preds

val_preds = evaluate(model, val_loader, val_labels)
test_preds = evaluate(model, test_loader, test_labels)

print("\nğŸ“Š Validation Results:")
print(classification_report(val_labels.cpu().numpy(), val_preds, target_names=le.classes_))

print("\nğŸ“Š Test Results:")
print(classification_report(test_labels.cpu().numpy(), test_preds, target_names=le.classes_))