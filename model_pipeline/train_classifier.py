import sys, os
import pandas as pd
from sklearn.model_selection import train_test_split

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from python_scraper.connect import get_conn
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
from sklearn.utils.class_weight import compute_class_weight
import numpy as np
import random
from sklearn.metrics import accuracy_score, f1_score
import itertools
from typing import Dict, List, Union, cast
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
from imblearn.over_sampling import RandomOverSampler
import shap


# set random seeds for reproducibility
SEED = 42
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
np.random.seed(SEED)
random.seed(SEED)

# ç¡®ä¿æ¯æ¬¡è®¡ç®—ä¸€è‡´ï¼ˆä½†ç•¥å¾®å½±å“æ€§èƒ½ï¼‰
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# ---------------------------
# experiment configurations
# ---------------------------

# æ¯æ‰¹è®­ç»ƒæ ·æœ¬çš„æ•°é‡ã€‚
# - æ•°å€¼è¶Šå¤§ï¼šè®­ç»ƒæ›´å¿«ã€æ¢¯åº¦æ›´ç¨³å®šï¼Œä½†å ç”¨æ˜¾å­˜æ›´é«˜ï¼›
# - æ•°å€¼è¶Šå°ï¼šæ¢¯åº¦æ›´æ–°æ›´é¢‘ç¹ï¼Œå¯èƒ½æ›´ç¨³å®šä½†è®­ç»ƒæ…¢ã€‚
# batch_size = [16, 32, 64]
# batch_size = [32, 64]
batch_size = [64]

# Embedding æ–‡ä»¶è·¯å¾„åˆ—è¡¨ã€‚
# æ¯ä¸ªæ–‡ä»¶å¯¹åº”ä¸€ç§æ–‡æœ¬ç‰¹å¾æŠ½å–ç­–ç•¥ï¼ˆä¸åŒ prompt æˆ– embedding æ¨¡å‹ï¼‰ï¼Œ
# ç¨‹åºä¼šä¾æ¬¡åŠ è½½è¿™äº›æ–‡ä»¶å¹¶è¿›è¡Œç‹¬ç«‹è®­ç»ƒè¯„ä¼°ï¼Œç”¨äºå¯¹æ¯”ç»“æœã€‚
embedding_paths = [
    "model_pipeline/embeddings/1ï¸âƒ£: only_exp_num_embeddings_new.pt",             # ä»…åŒ…å«â€œæ•°å­—+ç»éªŒâ€ä¿¡æ¯çš„å¥å‘é‡
    # "model_pipeline/embeddings/2ï¸âƒ£: exp_num+exp_embeddings.pt",   # æ•°å­—+ç»éªŒè¯å¤åˆç‰¹å¾
    # "model_pipeline/embeddings/3ï¸âƒ£: exp_num+salary_embeddings.pt",             # å¯ç”¨å…¨éƒ¨ç­›é€‰æ ‡å‡†ï¼ˆå®Œæ•´ç‰¹å¾ï¼‰
    # "model_pipeline/embeddings/4ï¸âƒ£: all_enabled_embeddings.pt",             # å…³é—­å…¨éƒ¨ç­›é€‰æ ‡å‡†ï¼ˆæ§åˆ¶ç»„ï¼‰
    # "model_pipeline/embeddings/5ï¸âƒ£: all_disabled_embeddings.pt",          # ä»…ä¿ç•™ç»éªŒç›¸å…³å¥å­ï¼ˆå»é™¤æ•°å€¼ï¼‰
    # "model_pipeline/embeddings/raw_jd_embeddings.pt",          # raw jd embeddings
]

# ---------------------------
# network configurations
# ---------------------------

# define the network architectures to try

network_configs = [

    # {"hidden_dims": [64], "dropout_rates": [0.3]},
    # {"hidden_dims": [128], "dropout_rates": [0.3]},
    # {"hidden_dims": [256], "dropout_rates": [0.3]},
    # {"hidden_dims": [512], "dropout_rates": [0.3]}, # best for 1 layer
    # {"hidden_dims": [1024], "dropout_rates": [0.3]},

    # {"hidden_dims": [256, 128], "dropout_rates": [0.3, 0.2]},
    # {"hidden_dims": [512, 256, 128], "dropout_rates": [0.3, 0.2, 0.1]},
    # {"hidden_dims": [512, 256, 128], "dropout_rates": [0.4, 0.3, 0.2]},
    # {"hidden_dims": [512, 256, 128], "dropout_rates": [0.5, 0.4, 0.3]},
    # {"hidden_dims": [1024, 512, 256, 128], "dropout_rates": [0.45,0.35,0.25,0.15]},
    {"hidden_dims": [1024, 512, 256, 128], "dropout_rates": [0.4, 0.3, 0.2, 0.1]}, # best
    # {"hidden_dims": [1024, 512, 256, 128], "dropout_rates": [0.35,0.3,0.25,0.2]}, # best

    # {"hidden_dims": [1024, 512, 256, 128], "dropout_rates": [0.3, 0.3, 0.3, 0.3]},
    # {"hidden_dims": [1024, 512, 256, 128], "dropout_rates": [0.2, 0.3, 0.4, 0.5]},
    # {"hidden_dims": [1024, 512, 256, 128], "dropout_rates": [0.5, 0.4, 0.3, 0.2]},
    # {"hidden_dims": [1024, 512, 256, 128, 64], "dropout_rates": [0.5, 0.4, 0.3, 0.2, 0.1]},
    # {"hidden_dims": [1024, 512, 256, 128], "dropout_rates": [0.6, 0.5, 0.4, 0.3]},
   
]

# æ˜¯å¦åœ¨è®¡ç®—æŸå¤±æ—¶å¯ç”¨ç±»åˆ«å¹³è¡¡æƒé‡ã€‚
# - Trueï¼šè‡ªåŠ¨è®¡ç®—æ¯ç±»æ ·æœ¬æ•°é‡ï¼Œæå‡å°‘æ•°ç±»ï¼ˆå¦‚ Juniorï¼‰çš„å­¦ä¹ æƒé‡ï¼›
# - Falseï¼šæ‰€æœ‰ç±»åˆ«æƒé‡ç›¸åŒã€‚
# balanced_class_weights = [True, False]
balanced_class_weights = [False]

# activation = ["relu", "gelu", "leakyrelu"]
# activation = ["relu", "leakyrelu"]
# activation = ["relu", "prelu"]
activation = ["relu"] # best
# activation = ["sigmoid"] # required for 1 layer

# use_batchnorm = [True, False]
use_batchnorm = [False]
# use_layernorm = [True, False]
use_layernorm = [False]



# ---------------------------
# setting for optimizer and learning rate
# ---------------------------

# é€‰æ‹©ä¼˜åŒ–ç®—æ³•ï¼š
# - "Adam"ï¼šé»˜è®¤æ¨èï¼Œæ”¶æ•›å¿«ä¸”ç¨³å®šï¼›
# - "AdamW"ï¼šå¸¦æƒé‡è¡°å‡çš„ Adamï¼Œæ³›åŒ–æ›´å¥½ï¼›
# - "SGD"ï¼šä¼ ç»Ÿéšæœºæ¢¯åº¦ä¸‹é™ï¼Œè®­ç»ƒæ›´æ…¢ä½†ç»“æœæ›´å¹³æ»‘ã€‚
# optimizer_name = ["Adam", "AdamW" , "SGD"]
# optimizer_name = ["Adam", "AdamW"]
optimizer_name = ["Adam"] # best

# å­¦ä¹ ç‡ï¼ˆLearning Rateï¼‰
# - æ§åˆ¶å‚æ•°æ›´æ–°å¹…åº¦ï¼›
# - 1e-3 æ˜¯è¾ƒå¸¸è§çš„é»˜è®¤å€¼ï¼›
# - å¯å°è¯• 5e-4 æˆ– 3e-3 åšæ•æ„Ÿåº¦å®éªŒã€‚
# learning_rate = [3e-4, 1e-3, 3e-3]
# learning_rate = [5e-4, 1e-3]
# learning_rate = [1e-3, 5e-4, 7e-4]
# learning_rate = [1e-3, 7e-4]
# learning_rate = [1e-3] # best
learning_rate = [0.001]


# æƒé‡è¡°å‡ï¼ˆL2 æ­£åˆ™é¡¹ï¼‰
# - é˜²æ­¢è¿‡æ‹Ÿåˆï¼›
# - é€šå¸¸ä¸ AdamW ä¸€èµ·ä½¿ç”¨ï¼›
# - å¯è®¾ä¸º 1e-4 è§‚å¯Ÿæ­£åˆ™åŒ–æ•ˆæœã€‚
# weight_decay = [0, 1e-4]
weight_decay = [0]

use_scheduler = [True]
# use_scheduler = [True, False]
# use_label_smoothing = [True, False]
use_label_smoothing = [False]


# ---------------------------
# training settings
# ---------------------------

# æœ€å¤§è®­ç»ƒè½®æ•°ã€‚
# - è®¾ç½®è¾ƒå¤§å€¼ï¼ˆå¦‚ 50ï¼‰ï¼Œå› ä¸ºæ—©åœæœºåˆ¶ä¼šè‡ªåŠ¨ç»ˆæ­¢è®­ç»ƒï¼›
# - å®é™…ä¸ä¼šè·‘æ»¡æ‰€æœ‰è½®æ•°ã€‚
epochs = 80

# æ—©åœæœºåˆ¶çš„â€œè€å¿ƒå€¼â€ã€‚
# - å½“éªŒè¯é›† loss è¿ç»­ patience è½®ä¸å†ä¸‹é™æ—¶åœæ­¢è®­ç»ƒï¼›
# - é€šå¸¸å– 5~10ã€‚
patience = 15


for path, net_cfg, opt, lr, wd, act, bn, sch, ul, uls in itertools.product(embedding_paths, network_configs, optimizer_name, learning_rate, weight_decay, activation, use_batchnorm, use_scheduler, use_layernorm, use_label_smoothing):

    print("\n" + "="*120)
    print(f"ğŸ”– Training with config: Batch Size=64, Embedding Path={path}, Hidden Dims={net_cfg['hidden_dims']}, Dropout Rates={net_cfg['dropout_rates']}, Balanced Weights=False, Optimizer={opt}, LR={lr}, WD={wd}, Activation={act}, BatchNorm={bn}, Scheduler={sch}, LayerNorm={ul}, Label Smoothing={uls}")
    print("="*120 + "\n")
    
    data = torch.load(path)

    # ---------------------------
    # load embeddings and labels
    # ---------------------------

    # æ ‡ç­¾ç¼–ç 
    le = LabelEncoder()
    train_labels = torch.tensor(le.fit_transform(data["train_labels"]), dtype=torch.long)
    val_labels   = torch.tensor(le.transform(data["val_labels"]), dtype=torch.long)
    test_labels  = torch.tensor(le.transform(data["test_labels"]), dtype=torch.long)

    train_emb = data["train_emb"]
    val_emb   = data["val_emb"]
    test_emb  = data["test_emb"]


    # ==========================================================
    # Oversampling
    # ==========================================================
    # ros = RandomOverSampler(random_state=42)

    # # æ³¨æ„ï¼šRandomOverSampler åªèƒ½å¤„ç† numpy æ ¼å¼
    # X_resampled, y_resampled = ros.fit_resample(train_emb.numpy(), train_labels.numpy())

    # # å†æŠŠå®ƒä»¬è½¬å› torch å¼ é‡
    # train_emb = torch.tensor(X_resampled, dtype=torch.float32)
    # train_labels = torch.tensor(y_resampled, dtype=torch.long)

    # print("âœ… After resampling:", {c: sum(train_labels.numpy() == c) for c in np.unique(train_labels.numpy())})

    # ---------------------------
    # data loaders
    # ---------------------------
    train_loader = DataLoader(TensorDataset(train_emb, train_labels), batch_size=64, shuffle=True)
    val_loader = DataLoader(TensorDataset(val_emb, val_labels), batch_size=64)
    test_loader = DataLoader(TensorDataset(test_emb, test_labels), batch_size=64)

    # ---------------------------
    # model definition
    # ---------------------------

    class MLPClassifier(nn.Module):
        def __init__(self, input_dim, num_classes,
                    hidden_dims=[256, 128],
                    dropout_rates=[0.3, 0.2],
                    activation="relu",          # å¯é€‰: relu / gelu / leakyrelu
                    use_batchnorm=False,
                    use_layernorm=False):       # æ˜¯å¦å¯ç”¨LayerNorm
            super().__init__()

            layers = []
            prev_dim = input_dim

            # åŠ¨æ€æ„å»ºæ¯å±‚
            for h_dim, d_rate in zip(hidden_dims, dropout_rates):
                layers.append(nn.Linear(prev_dim, h_dim))
                
                # âœ… è‹¥å¯ç”¨ BatchNormï¼Œæ”¾åœ¨æ¿€æ´»å‡½æ•°å‰
                if use_batchnorm:
                    layers.append(nn.BatchNorm1d(h_dim))
                if use_layernorm:
                    layers.append(nn.LayerNorm(h_dim))

                # âœ… åŠ¨æ€é€‰æ‹©æ¿€æ´»å‡½æ•°
                if activation.lower() == "relu":
                    layers.append(nn.ReLU())
                elif activation.lower() == "gelu":
                    layers.append(nn.GELU())
                elif activation.lower() == "leakyrelu":
                    layers.append(nn.LeakyReLU(negative_slope=0.01))
                elif activation.lower() == "prelu":
                    layers.append(nn.PReLU())
                elif activation.lower() == "sigmoid":
                    layers.append(nn.Sigmoid())
                else:
                    raise ValueError(f"Unsupported activation: {activation}")

                layers.append(nn.Dropout(d_rate))
                prev_dim = h_dim

            # è¾“å‡ºå±‚
            layers.append(nn.Linear(prev_dim, num_classes))

            self.net = nn.Sequential(*layers)

        def forward(self, x):
            return self.net(x)



    # -------------------------
    # initialize model
    # -------------------------
    input_dim = train_emb.shape[1]

    # baseline ç‰ˆæœ¬ï¼ˆä¸¤å±‚ï¼‰
    model = MLPClassifier(
        input_dim=input_dim,
        num_classes=len(le.classes_),
        hidden_dims=net_cfg['hidden_dims'],
        dropout_rates=net_cfg['dropout_rates'],
        activation=act,
        use_batchnorm=bn,
        use_layernorm=ul
    )

    # æˆ–è€…ï¼šæ›´æ·±ä¸€ç‚¹ç‰ˆæœ¬ï¼ˆé€‚åˆ e5-large-v2ï¼‰
    # model = MLPClassifier(
    #     input_dim=input_dim,
    #     num_classes=len(le.classes_),
    #     hidden_dims=[512, 256, 128],
    #     dropout_rates=[0.4, 0.3, 0.2]
    # )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # ---------------------------
    # define loss function
    # ---------------------------

    # bc = True

    # if bc:
    #     # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æƒé‡
    #     class_weights = compute_class_weight(
    #         class_weight='balanced',
    #         classes=np.unique(train_labels.numpy()),
    #         y=train_labels.numpy()
    #     )
    #     class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)
    # else:
    class_weights = None
    # ä½¿ç”¨åŠ æƒæŸå¤±å‡½æ•°
    if uls:
        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)
    else:
        criterion = nn.CrossEntropyLoss(weight=class_weights)

    # ---------------------------
    # define optimizer and scheduler
    # ---------------------------
    if opt == "Adam":
        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)
    elif opt == "AdamW":
        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)
    elif opt == "SGD":
        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)
    else:
        raise ValueError(f"Unsupported optimizer: {opt}")

    if sch:
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=4)

    # ---------------------------
    # training loop
    # ---------------------------

    # åˆå§‹åŒ–æœ€ä¼˜éªŒè¯é›† lossã€‚
    # - ç”¨æ— ç©·å¤§ï¼ˆinfï¼‰ä¿è¯ç¬¬ä¸€æ¬¡æ¯”è¾ƒæ—¶ä»»ä½•ç»“æœéƒ½èƒ½è¢«è§†ä¸ºâ€œæ›´ä¼˜â€ã€‚
    best_val_loss = float('inf')

    # æ—©åœè®¡æ•°å™¨ã€‚
    # - ç»Ÿè®¡éªŒè¯é›†è¿ç»­å¤šå°‘è½®æœªæ”¹å–„ï¼›
    # - è¾¾åˆ° patience å€¼åè§¦å‘æ—©åœã€‚
    patience_counter = 0


    for epoch in range(epochs):

        # start training
        model.train()
        total_loss = 0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        avg_train_loss = total_loss / len(train_loader)

        # evaluate on validation set
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for X_val, y_val in val_loader:
                X_val, y_val = X_val.to(device), y_val.to(device)
                outputs = model(X_val)
                loss = criterion(outputs, y_val)
                val_loss += loss.item()

        avg_val_loss = val_loss / len(val_loader)
        if sch:
            scheduler.step(avg_val_loss)

        # early stopping check
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            # save_dir = "model_pipeline"
            # save_path = os.path.join(save_dir, "best_model.pt")
            # torch.save(model.state_dict(), save_path)  # ä¿å­˜å½“å‰æœ€ä¼˜æ¨¡å‹
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"â¹ï¸ Early stopping triggered at epoch {epoch+1}")
                print(f"ğŸ§© Best Val Loss: {best_val_loss:.4f}")
                break
    else:
        print(f"âœ… Training finished. Best Val Loss: {best_val_loss:.4f}")

    # ---------------------------
    # 6ï¸âƒ£ è¯„ä¼°
    # ---------------------------
    def evaluate(model, loader, y_true):
        model.eval()
        preds = []
        with torch.no_grad():
            for X_batch, _ in loader:
                X_batch = X_batch.to(device)
                outputs = model(X_batch)
                preds.extend(outputs.argmax(dim=1).cpu().numpy())
        return preds

    val_preds = evaluate(model, val_loader, val_labels)
    test_preds = evaluate(model, test_loader, test_labels)

    # print("\nğŸ“Š Validation Results:")
    # print(classification_report(val_labels.cpu().numpy(), val_preds, target_names=le.classes_))

    # print("\nğŸ“Š Test Results:")
    # print(classification_report(test_labels.cpu().numpy(), test_preds, target_names=le.classes_))

    # val_f1 = f1_score(val_labels.cpu().numpy(), val_preds, average="macro")
    # test_f1 = f1_score(test_labels.cpu().numpy(), test_preds, average="macro")
    # val_acc = accuracy_score(val_labels.cpu().numpy(), val_preds)
    # test_acc = accuracy_score(test_labels.cpu().numpy(), test_preds)

    # print(f"\nâœ… Validation: Acc={val_acc:.3f}, Macro-F1={val_f1:.3f}")
    # print(f"ğŸ§© Test:       Acc={test_acc:.3f}, Macro-F1={test_f1:.3f}")

    # y_true = test_labels.cpu().numpy()
    # y_pred = test_preds
    # cm = confusion_matrix(y_true, y_pred, labels=range(len(le.classes_)))

    # disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)
    # disp.plot(cmap="Blues", values_format="d")
    # plt.title("Confusion Matrix for Test Set (Best Model)")
    # plt.show()

    all_emb = torch.cat([train_emb, val_emb, test_emb])
    all_labels = torch.cat([train_labels, val_labels, test_labels])

    model.eval()
    with torch.no_grad():
        outputs = model(all_emb.to(device))
        preds = torch.argmax(outputs, dim=1).cpu()

    print("\n=== Classification Report (All Data) ===")
    print(classification_report(all_labels, preds, target_names=le.classes_))

    # cm = confusion_matrix(all_labels, preds, labels=range(len(le.classes_)))
    # disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)
    # disp.plot(cmap="Blues", values_format="d")
    # plt.title("Confusion Matrix for All Data (Best Model)")
    # plt.show()



# load four types of embeddings for sensitivity analysis
data = torch.load("model_pipeline/embeddings/1ï¸âƒ£: only_exp_num_embeddings.pt")
parts = {
    "Job Title": data["test_title_emb"][:50],
    "Experience + Number": data["test_exp_num_emb"][:50],
    "Salary": data["test_salary_emb"][:50],
    "Experience + Skill": data["test_exp_skill_emb"][:50],
}

# define a prediction function for SHAP
def model_predict(x):
    x_t = torch.tensor(x, dtype=torch.float32).to(device)
    with torch.no_grad():
        y = model(x_t)
        y = torch.softmax(y, dim=1)
    return y.cpu().numpy()

# compute SHAP values for each part
mean_values = {}
for name, emb in parts.items():
    X = emb.cpu().numpy()
    explainer = shap.KernelExplainer(model_predict, X[:10])  
    shap_values = explainer.shap_values(X[:20])              
    mean_values[name] = np.mean(np.abs(shap_values[0]))      
    print(f"{name}: {mean_values[name]:.4f}")

# plot bar chart to compare the importance of four input types
plt.bar(list(mean_values.keys()), list(mean_values.values()), color="skyblue")
plt.ylabel("Mean |SHAP Value|")
plt.title("Sensitivity Analysis by Input Category")
plt.xticks(rotation=20)
plt.tight_layout()
plt.show()