import sys, os
import pandas as pd
from sklearn.model_selection import train_test_split

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from python_scraper.connect import get_conn
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
from sklearn.utils.class_weight import compute_class_weight
import numpy as np
import random

# å›ºå®šéšæœºç§å­ï¼Œä¿è¯ç»“æœå¯å¤ç°
SEED = 42
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
np.random.seed(SEED)
random.seed(SEED)

# ç¡®ä¿æ¯æ¬¡è®¡ç®—ä¸€è‡´ï¼ˆä½†ç•¥å¾®å½±å“æ€§èƒ½ï¼‰
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# ---------------------------
# âš™ï¸ å®éªŒå…¨å±€é…ç½®å‚æ•°
# ---------------------------

# æ¯æ‰¹è®­ç»ƒæ ·æœ¬çš„æ•°é‡ã€‚
# - æ•°å€¼è¶Šå¤§ï¼šè®­ç»ƒæ›´å¿«ã€æ¢¯åº¦æ›´ç¨³å®šï¼Œä½†å ç”¨æ˜¾å­˜æ›´é«˜ï¼›
# - æ•°å€¼è¶Šå°ï¼šæ¢¯åº¦æ›´æ–°æ›´é¢‘ç¹ï¼Œå¯èƒ½æ›´ç¨³å®šä½†è®­ç»ƒæ…¢ã€‚
batch_size = 32

# Embedding æ–‡ä»¶è·¯å¾„åˆ—è¡¨ã€‚
# æ¯ä¸ªæ–‡ä»¶å¯¹åº”ä¸€ç§æ–‡æœ¬ç‰¹å¾æŠ½å–ç­–ç•¥ï¼ˆä¸åŒ prompt æˆ– embedding æ¨¡å‹ï¼‰ï¼Œ
# ç¨‹åºä¼šä¾æ¬¡åŠ è½½è¿™äº›æ–‡ä»¶å¹¶è¿›è¡Œç‹¬ç«‹è®­ç»ƒè¯„ä¼°ï¼Œç”¨äºå¯¹æ¯”ç»“æœã€‚
embedding_paths = [
    "model_pipeline/embeddings/1ï¸âƒ£: only_exp_num_embeddings.pt",             # ä»…åŒ…å«â€œæ•°å­—+ç»éªŒâ€ä¿¡æ¯çš„å¥å‘é‡
    "model_pipeline/embeddings/2ï¸âƒ£: exp_num_and_experience_embeddings.pt",   # æ•°å­—+ç»éªŒè¯å¤åˆç‰¹å¾
    "model_pipeline/embeddings/3ï¸âƒ£: all_criteria_embeddings.pt",             # å¯ç”¨å…¨éƒ¨ç­›é€‰æ ‡å‡†ï¼ˆå®Œæ•´ç‰¹å¾ï¼‰
    "model_pipeline/embeddings/4ï¸âƒ£: all_disabled_embeddings.pt",             # å…³é—­å…¨éƒ¨ç­›é€‰æ ‡å‡†ï¼ˆæ§åˆ¶ç»„ï¼‰
    "model_pipeline/embeddings/5ï¸âƒ£: only_experience_embeddings.pt",          # ä»…ä¿ç•™ç»éªŒç›¸å…³å¥å­ï¼ˆå»é™¤æ•°å€¼ï¼‰
]

# ---------------------------
# ğŸ§  MLP ç½‘ç»œç»“æ„é…ç½®
# ---------------------------

# æ¯å±‚éšè—å±‚çš„ç¥ç»å…ƒæ•°é‡ã€‚
# - æ•°é‡è¶Šå¤šï¼Œæ¨¡å‹å®¹é‡è¶Šå¤§ï¼ˆèƒ½å­¦ä¹ æ›´å¤æ‚çš„ç‰¹å¾ï¼‰ï¼›
# - ä½†è¿‡å¤§å®¹æ˜“è¿‡æ‹Ÿåˆã€‚
hidden_dims = [256, 128]

# æ¯å±‚å¯¹åº”çš„ dropout æ¯”ä¾‹ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆã€‚
# - dropout=0.3 è¡¨ç¤ºéšæœºä¸¢å¼ƒ 30% çš„ç¥ç»å…ƒï¼›
# - é€šå¸¸å‰å±‚å¤§ã€åå±‚å°ã€‚
dropout_rates = [0.3, 0.2]

# æ˜¯å¦åœ¨è®¡ç®—æŸå¤±æ—¶å¯ç”¨ç±»åˆ«å¹³è¡¡æƒé‡ã€‚
# - Trueï¼šè‡ªåŠ¨è®¡ç®—æ¯ç±»æ ·æœ¬æ•°é‡ï¼Œæå‡å°‘æ•°ç±»ï¼ˆå¦‚ Juniorï¼‰çš„å­¦ä¹ æƒé‡ï¼›
# - Falseï¼šæ‰€æœ‰ç±»åˆ«æƒé‡ç›¸åŒã€‚
balanced_class_weights = True

# ---------------------------
# ğŸ§® ä¼˜åŒ–å™¨é…ç½®
# ---------------------------

# é€‰æ‹©ä¼˜åŒ–ç®—æ³•ï¼š
# - "Adam"ï¼šé»˜è®¤æ¨èï¼Œæ”¶æ•›å¿«ä¸”ç¨³å®šï¼›
# - "AdamW"ï¼šå¸¦æƒé‡è¡°å‡çš„ Adamï¼Œæ³›åŒ–æ›´å¥½ï¼›
# - "SGD"ï¼šä¼ ç»Ÿéšæœºæ¢¯åº¦ä¸‹é™ï¼Œè®­ç»ƒæ›´æ…¢ä½†ç»“æœæ›´å¹³æ»‘ã€‚
optimizer_name = "Adam"

# å­¦ä¹ ç‡ï¼ˆLearning Rateï¼‰
# - æ§åˆ¶å‚æ•°æ›´æ–°å¹…åº¦ï¼›
# - 1e-3 æ˜¯è¾ƒå¸¸è§çš„é»˜è®¤å€¼ï¼›
# - å¯å°è¯• 5e-4 æˆ– 3e-3 åšæ•æ„Ÿåº¦å®éªŒã€‚
learning_rate = 1e-3

# æƒé‡è¡°å‡ï¼ˆL2 æ­£åˆ™é¡¹ï¼‰
# - é˜²æ­¢è¿‡æ‹Ÿåˆï¼›
# - é€šå¸¸ä¸ AdamW ä¸€èµ·ä½¿ç”¨ï¼›
# - å¯è®¾ä¸º 1e-4 è§‚å¯Ÿæ­£åˆ™åŒ–æ•ˆæœã€‚
weight_decay = 0

# ---------------------------
# â±ï¸ è®­ç»ƒæ§åˆ¶å‚æ•°
# ---------------------------

# æœ€å¤§è®­ç»ƒè½®æ•°ã€‚
# - è®¾ç½®è¾ƒå¤§å€¼ï¼ˆå¦‚ 50ï¼‰ï¼Œå› ä¸ºæ—©åœæœºåˆ¶ä¼šè‡ªåŠ¨ç»ˆæ­¢è®­ç»ƒï¼›
# - å®é™…ä¸ä¼šè·‘æ»¡æ‰€æœ‰è½®æ•°ã€‚
epochs = 50

# æ—©åœæœºåˆ¶çš„â€œè€å¿ƒå€¼â€ã€‚
# - å½“éªŒè¯é›† loss è¿ç»­ patience è½®ä¸å†ä¸‹é™æ—¶åœæ­¢è®­ç»ƒï¼›
# - é€šå¸¸å– 5~10ã€‚
patience = 10

# åˆå§‹åŒ–æœ€ä¼˜éªŒè¯é›† lossã€‚
# - ç”¨æ— ç©·å¤§ï¼ˆinfï¼‰ä¿è¯ç¬¬ä¸€æ¬¡æ¯”è¾ƒæ—¶ä»»ä½•ç»“æœéƒ½èƒ½è¢«è§†ä¸ºâ€œæ›´ä¼˜â€ã€‚
best_val_loss = float('inf')

# æ—©åœè®¡æ•°å™¨ã€‚
# - ç»Ÿè®¡éªŒè¯é›†è¿ç»­å¤šå°‘è½®æœªæ”¹å–„ï¼›
# - è¾¾åˆ° patience å€¼åè§¦å‘æ—©åœã€‚
patience_counter = 0


for path in embedding_paths:
    data = torch.load(path)

    # ---------------------------
    # load embeddings and labels
    # ---------------------------

    # æ ‡ç­¾ç¼–ç 
    le = LabelEncoder()
    train_labels = torch.tensor(le.fit_transform(data["train_labels"]), dtype=torch.long)
    val_labels   = torch.tensor(le.transform(data["val_labels"]), dtype=torch.long)
    test_labels  = torch.tensor(le.transform(data["test_labels"]), dtype=torch.long)

    train_emb = data["train_emb"]
    val_emb   = data["val_emb"]
    test_emb  = data["test_emb"]

    # ---------------------------
    # æ„å»º DataLoader
    # ---------------------------
    train_loader = DataLoader(TensorDataset(train_emb, train_labels), batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(TensorDataset(val_emb, val_labels), batch_size=batch_size)
    test_loader = DataLoader(TensorDataset(test_emb, test_labels), batch_size=batch_size)

    # ---------------------------
    # å®šä¹‰ç®€å• MLP æ¨¡å‹
    # ---------------------------

    class MLPClassifier(nn.Module):
        def __init__(self, input_dim, num_classes, hidden_dims=[256, 128], dropout_rates=[0.3, 0.2]):
            """
            input_dim: è¾“å…¥ embedding çš„ç»´åº¦ï¼ˆä¾‹å¦‚ e5-large-v2 è¾“å‡ºæ˜¯ 1024ï¼‰
            num_classes: åˆ†ç±»æ•°ï¼ˆæ¯”å¦‚ 3ï¼‰
            hidden_dims: æ¯å±‚éšè—å±‚ç¥ç»å…ƒæ•°é‡åˆ—è¡¨ï¼Œä¾‹å¦‚ [512,256,128]
            dropout_rates: æ¯å±‚å¯¹åº”çš„ dropout æ¯”ä¾‹ï¼Œä¾‹å¦‚ [0.4,0.3,0.2]
            """
            super().__init__()

            # åŠ¨æ€æ„å»ºå¤šå±‚ MLP ç»“æ„
            layers = []
            prev_dim = input_dim

            for h_dim, d_rate in zip(hidden_dims, dropout_rates):
                layers.append(nn.Linear(prev_dim, h_dim))
                layers.append(nn.ReLU())
                layers.append(nn.Dropout(d_rate))
                prev_dim = h_dim

            # è¾“å‡ºå±‚
            layers.append(nn.Linear(prev_dim, num_classes))

            # å°è£…æˆé¡ºåºç½‘ç»œ
            self.net = nn.Sequential(*layers)

        def forward(self, x):
            return self.net(x)


    # -------------------------
    # å®šä¹‰æ¨¡å‹ç»“æ„
    # -------------------------
    input_dim = train_emb.shape[1]

    # baseline ç‰ˆæœ¬ï¼ˆä¸¤å±‚ï¼‰
    model = MLPClassifier(
        input_dim=input_dim,
        num_classes=len(le.classes_),
        hidden_dims=hidden_dims,
        dropout_rates=dropout_rates
    )

    # æˆ–è€…ï¼šæ›´æ·±ä¸€ç‚¹ç‰ˆæœ¬ï¼ˆé€‚åˆ e5-large-v2ï¼‰
    # model = MLPClassifier(
    #     input_dim=input_dim,
    #     num_classes=len(le.classes_),
    #     hidden_dims=[512, 256, 128],
    #     dropout_rates=[0.4, 0.3, 0.2]
    # )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # ---------------------------
    # å®šä¹‰æŸå¤±å‡½æ•°
    # ---------------------------

    if balanced_class_weights:
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æƒé‡
        class_weights = compute_class_weight(
            class_weight='balanced',
            classes=np.unique(train_labels.numpy()),
            y=train_labels.numpy()
        )
        class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)
    else:
        class_weights = None

    # ä½¿ç”¨åŠ æƒæŸå¤±å‡½æ•°
    criterion = nn.CrossEntropyLoss(weight=class_weights)

    # ---------------------------
    # åˆ›å»ºä¼˜åŒ–å™¨
    # ---------------------------
    if optimizer_name == "Adam":
        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    elif optimizer_name == "AdamW":
        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    elif optimizer_name == "SGD":
        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)
    else:
        raise ValueError(f"Unsupported optimizer: {optimizer_name}")

    # ---------------------------
    # è®­ç»ƒå¾ªç¯
    # ---------------------------
    for epoch in range(epochs):
        # ====== è®­ç»ƒé˜¶æ®µ ======
        model.train()
        total_loss = 0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        avg_train_loss = total_loss / len(train_loader)

        # ====== éªŒè¯é˜¶æ®µ ======
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for X_val, y_val in val_loader:
                X_val, y_val = X_val.to(device), y_val.to(device)
                outputs = model(X_val)
                loss = criterion(outputs, y_val)
                val_loss += loss.item()

        avg_val_loss = val_loss / len(val_loader)

        print(f"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}")

        # ====== æ—©åœé€»è¾‘ ======
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            # save_dir = "model_pipeline"
            # save_path = os.path.join(save_dir, "best_model.pt")
            # torch.save(model.state_dict(), save_path)  # ä¿å­˜å½“å‰æœ€ä¼˜æ¨¡å‹
        else:
            patience_counter += 1
            print(f"âš ï¸ No improvement for {patience_counter} epoch(s)")
            if patience_counter >= patience:
                print("â¹ï¸ Early stopping triggered!")
                break
    # ---------------------------
    # 6ï¸âƒ£ è¯„ä¼°
    # ---------------------------
    def evaluate(model, loader, y_true):
        model.eval()
        preds = []
        with torch.no_grad():
            for X_batch, _ in loader:
                X_batch = X_batch.to(device)
                outputs = model(X_batch)
                preds.extend(outputs.argmax(dim=1).cpu().numpy())
        return preds

    val_preds = evaluate(model, val_loader, val_labels)
    test_preds = evaluate(model, test_loader, test_labels)

    print("\nğŸ“Š Validation Results:")
    print(classification_report(val_labels.cpu().numpy(), val_preds, target_names=le.classes_))

    print("\nğŸ“Š Test Results:")
    print(classification_report(test_labels.cpu().numpy(), test_preds, target_names=le.classes_))