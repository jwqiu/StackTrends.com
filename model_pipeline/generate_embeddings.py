import sys, os
import pandas as pd
from sklearn.model_selection import train_test_split
import re

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from python_scraper.connect import get_conn
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
from sentence_transformers import SentenceTransformer
from tqdm import tqdm

# ---------------------------
# 1ï¸âƒ£ è¯»å–æ•°æ®åº“æ•°æ®
# ---------------------------
conn = get_conn()
verify_cur = conn.cursor()
verify_cur.execute("""
    SELECT job_id, job_title, job_des, job_level
    FROM jobs_filtered
    WHERE job_level IS NOT NULL;
""")
rows = verify_cur.fetchall()
# å®‰å…¨æ£€æŸ¥
if verify_cur.description is None:
    raise RuntimeError("âš ï¸ SQL æ‰§è¡Œå¤±è´¥æˆ–æ²¡æœ‰è¿”å›ç»“æœï¼Œè¯·æ£€æŸ¥å­—æ®µåæ˜¯å¦æ­£ç¡®ã€‚")
colnames = [desc[0] for desc in verify_cur.description]
verify_cur.close()
conn.close()

df = pd.DataFrame(rows, columns=colnames)
print(f"æ€»æ ·æœ¬æ•°: {len(df)}")
print(df['job_level'].value_counts())

# ---------------------------
# 2ï¸âƒ£ æŒ‰ç±»åˆ«åˆ†å±‚åˆ’åˆ†æ•°æ®é›†
# ---------------------------
# å…ˆåˆ’åˆ† train (0.6) vs temp (0.4)
train_df, temp_df = train_test_split(
    df,
    test_size=0.4,
    stratify=df['job_level'],
    random_state=42
)

# å†åˆ’åˆ† temp -> val/test å„ 0.2
val_df, test_df = train_test_split(
    temp_df,
    test_size=0.5,
    stratify=temp_df['job_level'],
    random_state=42
)

# ---------------------------
# 3ï¸âƒ£ æ£€æŸ¥æ¯ä¸ªå­é›†çš„åˆ†å¸ƒ
# ---------------------------
def show_distribution(label, data):
    print(f"\nğŸ“Š {label} é›†ç±»åˆ«åˆ†å¸ƒ:")
    print(data['job_level'].value_counts())

show_distribution("Train", train_df)
show_distribution("Validation", val_df)
show_distribution("Test", test_df)

# ---------------------------
# 4ï¸âƒ£ å¯é€‰ï¼šä¿å­˜æˆ–ä¼ é€’å˜é‡
# ---------------------------
# train_df, val_df, test_df å¯ç›´æ¥ç”¨äºæ¨¡å‹è®­ç»ƒ
# ä¾‹å¦‚ï¼š
# train_df.to_csv("train_data.csv", index=False)
# val_df.to_csv("val_data.csv", index=False)
# test_df.to_csv("test_data.csv", index=False)

# ---------------------------
# 1ï¸âƒ£ æ•°æ®å‡†å¤‡
# ---------------------------
# å‡è®¾ä½ å·²æœ‰ train_df, val_df, test_df
# æ¯ä¸ªæœ‰åˆ—: ['job_title', 'job_des', 'job_level']
# å¦‚æœæ–‡æœ¬æ˜¯ job_title + job_des æ‹¼ä¸€èµ·æ›´å¥½

# pattern to split sentences
SENT_SPLIT = re.compile(r'(?<=[.!?:;Â·â€¢|])\s+|\n+')

# pattern to detect sentences that contain the word "experience" along with a number

# å…ˆå°è¯•åŒ¹é…æ¡ä»¶ 1ï¼ˆæ•°å­— + experienceï¼‰
pattern_experience_num = re.compile(
    r'^(?!.*\$).*?(?:'
    r'\b(?:\d+|one|two|three|four|five|six|seven|eight|nine|ten|several)\b[^.]{0,50}\bexperience\b'
    r'|\bexperience\b[^.]{0,50}\b(?:\d+|one|two|three|four|five|six|seven|eight|nine|ten|several)\b'
    r')',
    re.IGNORECASE
)

# å†å‡†å¤‡æ¡ä»¶ 2ï¼ˆå²—ä½çº§åˆ«ç±»ï¼‰
pattern_level = re.compile(
    r'\bintern(ship)?\b'
    r'|\bgraduate|graduat(e|es|ed|ing)\b'
    r'|\bjunior\b'
    r'|\bexperienced\b',
    re.IGNORECASE
)

# Pattern to detect sentences that contain the word "experience"
# pattern_experience     = re.compile(r'\bexperience\b', re.IGNORECASE) 
pattern_experience = re.compile(
    r'\b('
    r'experience|experienced|'
    r'skill|skills'
    r')\b',
    re.IGNORECASE
)

# Pattern to detect sentences that contain the word "salary" along with a dollar sign
pattern_salary = re.compile(
    r'(?=.*\b(?:salary|compensation|pay)\b)(?=.*\$)',
    re.IGNORECASE
) 

def extract_requirement_text(
    text: str,
    use_experience: bool = False,
    use_experience_num: bool = True,
    use_salary: bool = False,
) -> str:

    # split job_des into sentences
    sentences = []
    for s in SENT_SPLIT.split(text):
        s = s.strip()
        if s:
            sentences.append(s)

    # split long sentences by commas
    refined_sentences = []
    for s in sentences:
        if len(s.split()) > 100:
            parts = re.split(r'(?<=,)\s*', s)
            clean_parts =[]
            for x in parts:
                x = x.strip()
                if x:
                    clean_parts.append(x)
            refined_sentences.extend(clean_parts)
        else:
            refined_sentences.append(s)

    sentences = refined_sentences

    # define three lists to store different types of matched sentences
    exp_num_sents, salary_sents, exp_sents = [], [], []

    # find sentences that match the patterns
    for s in sentences:
        # skip questions, like what experience do you have ?
        if s.strip().endswith('?'):
            continue
        # check for experience with numbers
        if use_experience_num:
            if pattern_experience_num.search(s):        # âœ… ä¼˜å…ˆåŒ¹é…æ¡ä»¶ 1ï¼ˆæ•°å­— + experienceï¼‰
                exp_num_sents.append(s)
                continue
            elif pattern_level.search(s):               # âœ… å¦‚æœæ¡ä»¶ 1 æ²¡å‘½ä¸­ï¼Œå†å°è¯•æ¡ä»¶ 2ï¼ˆintern/junior/graduate/experiencedï¼‰
                exp_num_sents.append(s)
                continue
        # check for salary information
        if use_salary and pattern_salary.search(s):
            salary_sents.append(s)
            continue
        # check for experience without numbers
        if use_experience and pattern_experience.search(s):
            exp_sents.append(s)

    # if none found, relax the condition to just contain "experience"
    if not (exp_num_sents or salary_sents or exp_sents):
        exp_sents = [s for s in sentences if pattern_experience.search(s) and not s.strip().endswith('?')]

    exp_num_text = ""
    salary_text = ""
    exp_text = ""

    # add labels to the extracted parts
    if exp_num_sents:
        exp_num_text = "[Years of experience required] " + " ".join(exp_num_sents)
    if salary_sents:
        salary_text = "[Salary details] " + " ".join(salary_sents)
    if exp_sents:
        exp_text = "[Experience and Skills] " + " ".join(exp_sents)

    # combine the extracted parts
    parts = [exp_num_text, salary_text, exp_text]
    matched = " ".join(p for p in parts if p).strip()

    return matched

train_df['job_des_filtered'] = train_df['job_des'].fillna('').apply(extract_requirement_text).tolist()
val_df['job_des_filtered'] = val_df['job_des'].fillna('').apply(extract_requirement_text).tolist()
test_df['job_des_filtered'] = test_df['job_des'].fillna('').apply(extract_requirement_text).tolist()

train_df["title_plus_des"] = (
    "This job title is " + train_df["job_title"].astype(str) + ". " +
    train_df["job_des_filtered"].astype(str)
)
val_df["title_plus_des"] = (
    "This job title is " + val_df["job_title"].astype(str) + ". " +
    val_df["job_des_filtered"].astype(str)
)
test_df["title_plus_des"] = (
    "This job title is " + test_df["job_title"].astype(str) + ". " +
    test_df["job_des_filtered"].astype(str)
)

train_texts = train_df["title_plus_des"].tolist()
val_texts = val_df["title_plus_des"].tolist()
test_texts = test_df["title_plus_des"].tolist()

# ---------------------------
# 2ï¸âƒ£ æ–‡æœ¬å‘é‡åŒ– (Embedding)
# ---------------------------
print("ğŸ”¹ Encoding texts with SentenceTransformer ...")
model_emb = SentenceTransformer('intfloat/e5-large-v2',device="cpu")

train_emb = torch.from_numpy(model_emb.encode(train_texts, batch_size=32, show_progress_bar=True))
val_emb = torch.from_numpy(model_emb.encode(val_texts, batch_size=32, show_progress_bar=True))
test_emb = torch.from_numpy(model_emb.encode(test_texts, batch_size=32, show_progress_bar=True))

# ---------------------------
# 3ï¸âƒ£ save embeddings and labels
# ---------------------------
os.makedirs("model_pipeline/embeddings", exist_ok=True)
torch.save({
    "train_emb": train_emb,
    "val_emb": val_emb,
    "test_emb": test_emb,
    "train_labels": train_df["job_level"].tolist(),
    "val_labels": val_df["job_level"].tolist(),
    "test_labels": test_df["job_level"].tolist(),
}, "model_pipeline/embeddings/embeddings.pt")

print("âœ… Embeddings saved to model_pipeline/embeddings/embeddings.pt")