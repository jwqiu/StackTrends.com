import os, sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))) # æŠŠé¡¹ç›®æ ¹ç›®å½•åŠ å…¥æ¨¡å—æœç´¢è·¯å¾„
from connect import get_conn
import torch
from model_pipeline.generate_embeddings import extract_requirement_text
from sentence_transformers import SentenceTransformer
import pandas as pd

print("ğŸš€ Running generate_job_embeddings.py")
print("ğŸ“‚ Current file path:", __file__)
print("ğŸ“ Current working dir:", os.getcwd())

model_emb = SentenceTransformer("intfloat/e5-large-v2")
os.makedirs("python_scraper/embeddings", exist_ok=True)

conn = get_conn()
cursor = conn.cursor()
cursor.execute("SELECT job_id, job_title, job_des, job_level FROM jobs WHERE job_level = 'Other'")
rows = cursor.fetchall()
colnames = []
if cursor.description is not None:
    for desc in cursor.description:
        colnames.append(desc[0])

df = pd.DataFrame(rows, columns=colnames)
print(f"Loaded {len(df)} 'Other' level job postings.")

configs = [
    {"name": "1ï¸âƒ£: only_exp_num", "use_experience_num": True, "use_salary": False, "use_experience": False},
    # {"name": "2ï¸âƒ£: exp_num+exp", "use_experience_num": True, "use_salary": False, "use_experience": True},
    # {"name": "3ï¸âƒ£: exp_num+salary", "use_experience_num": True, "use_salary": True, "use_experience": False},
    # {"name": "4ï¸âƒ£: all_enabled", "use_experience_num": True, "use_salary": True, "use_experience": True},
    # {"name": "5ï¸âƒ£: all_disabled", "use_experience_num": False, "use_salary": False, "use_experience": False},
]

for cfg in configs:
    print(f"Processing configuration: {cfg['name']}")

    df['job_des_filtered'] = df['job_des'].fillna('').apply(
        lambda x: extract_requirement_text(
            x,
            use_experience=cfg["use_experience"],
            use_experience_num=cfg["use_experience_num"],
            use_salary=cfg["use_salary"]
        )
    ).tolist()

    df["title_plus_des"] = (
        "This job title is " + df["job_title"].astype(str) + ". " +
        df["job_des_filtered"].astype(str)
    )

    text = df["title_plus_des"].tolist()
    embeddings = torch.from_numpy(model_emb.encode(text, batch_size=32, show_progress_bar=True))

    torch.save(
        {
            "embeddings":embeddings
        },
        f"python_scraper/embeddings/{cfg['name']}.pt"
    )

    print(f"Saved embeddings to python_scraper/embeddings/{cfg['name']}.pt")

# input_dim = train_emb.shape[1]
#
# model=MLPClassifier(
#     input_dim=input_dim,
#     num_classes=len(le.classes_),
#     hidden_dims=net_cfg['hidden_dims'],
#     dropout_rates=net_cfg['dropout_rates'],
#     activation=act,
#     use_batchnorm=bn,
#     use_layernorm=uln,
# )