import os, sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))) # æŠŠé¡¹ç›®æ ¹ç›®å½•åŠ å…¥æ¨¡å—æœç´¢è·¯å¾„
import torch
from model_pipeline.train_classifier import MLPClassifier
import pandas as pd
from python_scraper.connect import get_conn


embedding_paths = {
    "only_exp_num": "python_scraper/embeddings/1ï¸âƒ£: only_exp_num_embeddings.pt",
    # "exp_num+exp": "python_scraper/embeddings/2ï¸âƒ£: exp_num+exp_embeddings.pt",

}

for name, path in embedding_paths.items():

    # ---------------------------
    # 1ï¸âƒ£ åŠ è½½ embedding æ–‡ä»¶
    # ---------------------------
    data = torch.load(path)
    X_embeddings = data['embeddings']
    input_dim = X_embeddings.shape[1]
    job_ids = data["job_id"]
    job_titles = data["job_title"]
    raw_texts = data["raw_texts"]

    # ---------------------------
    # 2ï¸âƒ£ åˆ›å»ºæ¨¡å‹å®ä¾‹
    # ---------------------------
    model = MLPClassifier(
        input_dim=input_dim,
        num_classes=3,
        hidden_dims=[1024, 512, 256, 128],
        dropout_rates=[0.4, 0.3, 0.2, 0.1],
        activation="relu",
        use_batchnorm=False,
        use_layernorm=False
    )

    # ---------------------------
    # 3ï¸âƒ£ åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
    # ---------------------------
    model.load_state_dict(torch.load("model_pipeline/best_model.pt", map_location=torch.device('cpu')))
    model.eval()
    print(f"âœ… Loaded model for embeddings from {path} successfully.")

    # ---------------------------
    # 4ï¸âƒ£ æ¨¡å‹é¢„æµ‹
    # ---------------------------

    with torch.no_grad():
        logits = model(X_embeddings)
        probs = torch.softmax(logits, dim=1)
        preds = torch.argmax(probs, dim=1).cpu().numpy()

    # ---------------------------
    # 5ï¸âƒ£ æ˜ å°„ç±»åˆ«åç§°
    # ---------------------------
    label_map = {0: "Intermediate", 1: "Junior", 2: "Senior"}  # è¦ä¸è®­ç»ƒæ—¶ä¸€è‡´
    pred_labels = [label_map[i] for i in preds]

    # ---------------------------
    # 6ï¸âƒ£ ä¿å­˜ç»“æœåˆ° CSV
    # ---------------------------
    df = pd.DataFrame({
        "job_id": job_ids,
        "job_title": job_titles,
        "raw_text": raw_texts,
        "pred_label": pred_labels,
        "pred_idx": preds
    })

    # save_path = f"python_scraper/predictions/{name}_predictions.csv"
    # os.makedirs(os.path.dirname(save_path), exist_ok=True)
    # df.to_csv(save_path, index=False, encoding="utf-8-sig")
    # print(f"ğŸ’¾ Saved predictions to {save_path}")

    print(df.head(30))

    conn = get_conn()
    cursor = conn.cursor()

    for _, row in df.iterrows():
        job_id = row["job_id"]
        job_level = row["pred_label"]

        try:
            cursor.execute(
                """
                UPDATE jobs
                SET job_level = %s
                WHERE job_id = %s;
                """,
                (job_level, job_id),
            )
        except Exception as e:
            print(f"âš ï¸ æ›´æ–° job_id={job_id} æ—¶å‡ºé”™: {e}")
            conn.rollback()
    
    conn.commit()
    cursor.close()
    conn.close()
    print("âœ… All updates committed successfully.")